{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2878: DtypeWarning: Columns (0,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82848\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "2563442\n",
      "1868141\n",
      "Read 1868141 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "lyric 167894\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "from string import punctuation\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf8')\n",
    "\n",
    "\n",
    "from keyword_extraction import RakeKeywordExtractor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rake = RakeKeywordExtractor()\n",
    "print(device)\n",
    "\n",
    "# Class lang: Convert each word to index and convert each index to word\n",
    "# WordEmbedding - this structure can be viewed as a corpus \n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2 #count SOS and EOS\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        if sentence:\n",
    "             for word in sentence.split(' '):\n",
    "                self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "#Prepare for training data\n",
    "#Convert all Unicode to ASCII, normalize the word: make everything lowercase, trim most punctuations\n",
    "# def unicodeToAscii(s):\n",
    "#     return ''.join(\n",
    "#         c for c in unicodedata.normalize('NFD', s)\n",
    "#         if unicodedata.category(c) != 'Mn'\n",
    "#     )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = ''.join(c for c in s if c not in punctuation)\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "#Load data, process data into pairs(pair[0] is current sentence, pair[1] is the next sentence)\n",
    "#Pair[0] is the input sentence, pair[1] is the ground truth\n",
    "def loadData():\n",
    "    pf = open('rock_eng_clean.csv', encoding='utf-8')\n",
    "    df = pd.read_csv(pf, delimiter=',', names=['index', 'remix', 'yead', 'singer', 'type', 'lyric','keywords','scores'])\n",
    "    lyrics = df['lyric'].tolist()\n",
    "    keywords = df['keywords'].tolist()\n",
    "    scores = df['scores'].tolist()\n",
    "    print(len(keywords))\n",
    "    \n",
    "    pairs = []\n",
    "    cnt = 0\n",
    "    songs_len = len(lyrics)\n",
    "    total_line = 0\n",
    "    for i in range(1, len(lyrics)):\n",
    "        if(i%1000==0): print(i)\n",
    "#     for i in range(1, 1000):\n",
    "        lyrics_lines = lyrics[i].decode('utf-8').split('\\n')\n",
    "        keywords_lines = keywords[i].decode('utf-8').split('\\n')\n",
    "        scores_lines = scores[i].decode('utf-8').split('\\n')\n",
    "        lyric_line_num = len(lyrics_lines)\n",
    "#         keywords_line_num = len(keywords_lines)\n",
    "#         in each song\n",
    "        isFirstLine = True\n",
    "#     sometimes in the dataset there are 49 lines of lyrics, only 1 keywords\n",
    "        if(len(keywords_lines)<len(lyrics_lines)): continue\n",
    "#         print(len(lyrics_lines), len(keywords_lines))\n",
    "        for j in range(lyric_line_num):\n",
    "            total_line+=1\n",
    "            pair = []\n",
    "            current_lyric = lyrics_lines[j]\n",
    "            current_keyword = keywords_lines[j]\n",
    "            current_score = scores_lines[j]\n",
    "            if(len(current_score)>0 and round(float(current_score))>20):\n",
    "                if(isFirstLine):\n",
    "                    pair.append(normalizeString(current_keyword).encode('utf-8'))\n",
    "                    isFirstLine = False\n",
    "                else:\n",
    "                    pair.append(normalizeString(current_keyword+' '+previous_lyric).encode('utf-8'))\n",
    "                pair.append(normalizeString(current_lyric).encode('utf-8'))\n",
    "                pairs.append(pair)\n",
    "            previous_lyric = current_lyric\n",
    "    print(total_line)\n",
    "    print(len(pairs))\n",
    "    return pairs\n",
    "\n",
    "#Define a prepareData function to process all the input sentence pairs\n",
    "def prepareData(lang):\n",
    "    pairs = loadData()\n",
    "    lang = Lang(lang)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        if pair: \n",
    "            lang.addSentence(pair[0])\n",
    "            lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(lang.name, lang.n_words)\n",
    "    return lang, pairs\n",
    "\n",
    "\n",
    "lang, pairs = prepareData('lyric')\n",
    "#randomly choose a pair, print pair[0] and pair[1]\n",
    "i = 0\n",
    "# while i<100:\n",
    "#     print(random.choice(pairs))\n",
    "#     i += 1\n",
    "\n",
    "#Define Encoder and Decoder\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "import torch.utils.data\n",
    "#Prepare for training data\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "def indexesFromWord(lang, word):\n",
    "    return [lang.word2index[word]]\n",
    "\n",
    "def tensorFromWord(lang, sentence):\n",
    "    indexes = indexesFromWord(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def evaluate_word(encoder, decoder, sentence, max_length=10):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromWord(lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "       # decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            #decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, max_length=10):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "       # decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            #decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(lang.n_words, hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, lang.n_words).to(device)\n",
    "encoder1.load_state_dict(torch.load('./encoder1_rock_e1'))\n",
    "decoder1.load_state_dict(torch.load('./decoder1_rock_e1'))\n",
    "\n",
    "def keyword_generator(sentence, num):\n",
    "    #keyword extraction\n",
    "    keywords = list(rake.sen_extract(sentence, word_num=num, incl_scores=True))\n",
    "    filter_keywords = []\n",
    "    for keyword in keywords:\n",
    "        if(keyword[1]>=10):\n",
    "            filter_keywords.append(keyword[0])\n",
    "    for keyword in filter_keywords:\n",
    "        try:\n",
    "            lang.word2index[keyword]\n",
    "        except KeyError:\n",
    "            filter_keywords.remove(keyword)\n",
    "    return filter_keywords\n",
    "\n",
    "def lyric_generator(keywords, sentence):\n",
    "    if keywords:\n",
    "        output_words = []\n",
    "        input_sen = normalizeString(sentence)\n",
    "        new_input = keywords[0] + \" \" + input_sen\n",
    "        output_word = evaluate(encoder1, decoder1, new_input)\n",
    "        output_words.append(output_word)\n",
    "        for i in range(1, len(keywords)):\n",
    "            new_sen = keywords[i] + \" \" + ' '.join(output_word)\n",
    "            sen = ' '.join(new_sen.split(' ')[:-1])\n",
    "            output_word = evaluate(encoder1, decoder1, sen)\n",
    "            output_words.append(output_word)\n",
    "\n",
    "        # encode utf-8\n",
    "        output = []\n",
    "        for sen in output_words:\n",
    "            output.append(list(map(lambda x: x, sen)))\n",
    "\n",
    "        return output\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def rock_generator(sentence):\n",
    "    keywords = keyword_generator(sentence, 4)\n",
    "    generated_lyric = lyric_generator(keywords, sentence)\n",
    "    if generated_lyric:      \n",
    "        key = []\n",
    "        for keyword in keywords:\n",
    "            key.append(keyword.encode('utf-8'))\n",
    "\n",
    "        print(\"input: \")\n",
    "        print(sentence)\n",
    "        print()\n",
    "        print(\"keywords:\")\n",
    "        print(key)\n",
    "        print()\n",
    "        print(\"output:\")\n",
    "        for out in generated_lyric:\n",
    "            print(' '.join(out))\n",
    "    else:\n",
    "        print(\"Error: We cannot extract keywords, please provide a more meaningful sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "I wanna touch ya, love ya, baby all night\n",
      "\n",
      "keywords:\n",
      "['touch', 'night', 'love', 'babi']\n",
      "\n",
      "output:\n",
      "You touch me where I can touch you <EOS>\n",
      "Im all right through the night <EOS>\n",
      "You left me were in the sun <EOS>\n",
      "You left a hole where we can go <EOS>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I wanna touch ya, love ya, baby all night\"\n",
    "rock_generator(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "You can leave with me or you could have the blues\n",
      "\n",
      "keywords:\n",
      "['could', 'blue', 'leav']\n",
      "\n",
      "output:\n",
      "You could feel the end <EOS>\n",
      "You will be the end of me <EOS>\n",
      "You left a little more <EOS>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"You can leave with me or you could have the blues\"\n",
    "rock_generator(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "today is a sunny day, I am so happy\n",
      "\n",
      "keywords:\n",
      "['sunni', 'happi', 'today', 'day']\n",
      "\n",
      "output:\n",
      "You are not to make it up <EOS>\n",
      "You gotta get away and I feel <EOS>\n",
      "Im not a way today <EOS>\n",
      "Im going to make it through <EOS>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"today is a sunny day, I am so happy\"\n",
    "rock_generator(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: We cannot extract keywords, please provide a more meaningful sentence\n"
     ]
    }
   ],
   "source": [
    "sentence = \"ha ha ha ha ha ha\"\n",
    "rock_generator(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p27)",
   "language": "python",
   "name": "conda_pytorch_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
