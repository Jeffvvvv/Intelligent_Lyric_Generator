{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "cuda\n",
      "23858\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "963591\n",
      "661730\n",
      "Read 661730 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "lyric 89280\n",
      "['matter A true believer love will overtake me', 'No matter where you are']\n",
      "['reckon Flowing on', 'Wat you fink you reckon on']\n",
      "['perfect Bossy angry', 'Isnt this a perfect love']\n",
      "['tambourin Someone save my save my tambourine', 'Tambourine']\n",
      "['amongst', 'Who is this guy who hears her voice amongst angers noise and knows her tears']\n",
      "['whoaaaa Because Youre The Baddest Chick i Know', 'Whoaa Whoaaaa']\n",
      "['bright Silent night holy night', 'All is calm all is bright']\n",
      "['bridg cause I wanna be dancin all night long', 'Bridge']\n",
      "['insid Youll turn to hide in silence', 'It hurts so much inside']\n",
      "['shake Once the music hits your system theres no way your gonna stop', 'Come on shake your body baby do the conga']\n",
      "['lalalalov Lalalalove me down down', 'Lalalalove me']\n",
      "['swallow On pepper tree', 'Hardened swallow whole hose']\n",
      "['somewher Down the lonely road', 'Somewhere down the lonely road']\n",
      "['better Weve come this far', 'It can only get better']\n",
      "['matter Go head and flaunt and drop it like its hot', 'No matter what you do hell forget me not']\n",
      "['though Id hate to lie my friend dont deny', 'Cause what you sees real even though you cant feel']\n",
      "['never Falling on my face while running in this race', 'Dont wanna ever look back say I never did that']\n",
      "['endless Just the way you like it', 'And in this dream of endless space']\n",
      "['believ Im not the only one', 'I believe in love']\n",
      "['without Just means youre without me', 'Means youre without me']\n",
      "['better But if you love me', 'Then its better to leave this way']\n",
      "['storm On the other side', 'Oh let the storm roll over me']\n",
      "['flower Dropdropdropdropping this thing back', 'This is for the time you gave me flowers']\n",
      "['everi Theres no control are you with me now', 'Your every wish will be done']\n",
      "['search Search your heart search your soul', 'Oh when you find me there youll search no more']\n",
      "['number I dont know what I want at this party thing dont get on', 'and then maybe then number one Ill be home']\n",
      "['togeth Five days since you laughed at me saying', 'Get that together come back and see me']\n",
      "['heart The sound of believer', 'The rain in her heart']\n",
      "['empti empty space', 'to fill an empty place']\n",
      "['system Dont want to retreat', 'Till Im part of the system']\n",
      "['bring You have always been my everything', 'You have opened my heart with the joy youve been bringing in']\n",
      "['never You think that theyre playing games', 'They leave and never act the same']\n",
      "['wonder I called youre cell n youre home n still I sit here alone', 'Boy you got me wondering where you at where you going n were you been']\n",
      "['choru Never be another', 'Chorus']\n",
      "['thing Got me breaking breaking all of my rules oh', 'Its all the little things that you do']\n",
      "['parti It aint nothing but a party', 'It aint nothing but a party']\n",
      "['crawl Its so hard to restraighten your back', 'If we crawl at two']\n",
      "['sleep And youll never get to feel the moment', 'Its like youre sleeping']\n",
      "['though And steal a kiss or two Just one kiss', 'Though he may be far away']\n",
      "['pollut A type of personal solution', 'We all have got our own pollution']\n",
      "['angel My angels are dancing with me tonight', 'The angels are dancing with me']\n",
      "['warhors Hundred plus through black and white', 'Warhorse warhead fuck em man']\n",
      "['outta Son keep the mic so ball with the Brown', 'Or get that ass outta town']\n",
      "['suppos Here I am and I stand so tall', 'Im just the way Im supposed to be']\n",
      "['right', 'Ill hold you down you come right now its all for you']\n",
      "['dugeundaeneun For Five Six georeogayo', 'Ireohke dugeundaeneun mameul geudaen anayo']\n",
      "['heart Why Im not free', 'And baby I bet my heart']\n",
      "['wonder Keeping my side of the bed warm', 'You shouldnt even have to wonder by now']\n",
      "['without Without a dream in my heart', 'Without a love of my own']\n",
      "['littl Because I dont want to be without you', 'There is so little to risk']\n",
      "['could Gone are the days of summer', 'We couldnt change it if we tried']\n",
      "['fight The Laws We Had Put Down Down O Baby', 'The Laws For The Fight O Baby']\n",
      "['believ From every doubt I hadIm finally free', 'Now I truly believe']\n",
      "['whole Take my hand take my hand', 'Take my whole life too life too']\n",
      "['sunlight You are my raindrop I am the sea', 'With you and Godwhos my sunlight I bloom and grow so beautifully']\n",
      "['hoopti I thank God for the bae he called and saved the day', 'Bout to hop up in the hooptie']\n",
      "['paper You can run baby', 'You see me in the paper']\n",
      "['right You may move to a different drum', 'Jump right in and take you place']\n",
      "['imposs Youre unreachable', 'Youre making loving impossible']\n",
      "['could Get witchu right after my shows', 'Baby I could give you ways']\n",
      "['disappear And whats the use in pretending', 'Lets make the smoke and mirrors disappear']\n",
      "['neutron Baby baby baby baby baby', 'Baby babys got a neutron bomb']\n",
      "['hollywood Hollywood Jit', 'Its getting money time for mr hollywood jit']\n",
      "['bloom There amongst the flowers', 'Blooming in the land']\n",
      "['think baby if i cant be with you my sweet babe', 'then i think im gonna take it out on me']\n",
      "['wouldnt prescribed a new man', 'cuz he wouldnt listen']\n",
      "['absenc Devotion dont feed me', 'Cos absence just leads lonely lovers to stray']\n",
      "['shipeund mot nae ijeot damyeo ugyeo daeneun mal ijen bye bye', 'na eotteok hajyo eonni ijen jam deulgo shipeunde']\n",
      "['temporari The way that people come and go', 'Through temporary lives']\n",
      "['husband Feeds the baby makes the breakfast gets the kids off to school', 'And her husbands always late for work']\n",
      "['realli', 'There really isnt much to do']\n",
      "['stand And darling darling stand by me', 'Oh stand by me']\n",
      "['prove You make me feel beautiful beautiful', 'When I have nothing left to prove']\n",
      "['harmoni Oooh this time Im telling you Im telling you', 'Fifth Harmony']\n",
      "['enough But its not enough for me', 'not enough for me']\n",
      "['understand I think I got the master plan', 'So understand this']\n",
      "['somebodi Tonight', 'Oh yeah yeah yeah I am somebody']\n",
      "['superstiti', 'Very superstitious writing on the wall']\n",
      "['romanc Come shining all through', 'Romance will always be so new']\n",
      "['monthold Idiot', 'Ill bet a buck he looks like monthold lox also and idiot']\n",
      "['suddenli And then youd come right back to me today', 'But suddenly its dawned on me']\n",
      "['yeaheyay This state of independence shall be', 'Say yeaheyay yeaheyo']\n",
      "['everi Wild and free I could feel the sun', 'Your every wish will be done']\n",
      "['countri Akon and Filapine', 'We running the country']\n",
      "['enough Shes addicted to my love', 'She cant get enough Just take it slow']\n",
      "['flawless Oh no', 'Im flawless']\n",
      "['chang So much to belong', 'Loves about to change my heart']\n",
      "['fulfil All my life Ive been so blue', 'But in that moment you fulfilled me']\n",
      "['without You are madly in love with', 'A life without a story']\n",
      "['would Im sorry if I missed you', 'I wouldnt want to lose you']\n",
      "['choru Got to find the release ooh', 'Chorus']\n",
      "['night We can follow the sun to the day light is gone', 'We can gaze up the sky till the night is over']\n",
      "['everi We are the happy people with invisible ropes around our necks', 'Every step has a meaning']\n",
      "['perfect When boy meets girl maybe tomorrow well try again', 'No complications in a perfect world']\n",
      "['georgia Southside but its all good we goin chill', 'ATL Georgia what do we do for ya']\n",
      "['immort To be an immortal one must not devour', 'To be an immortal we gotta know whats right']\n",
      "['chang And love can change wonderful', 'Love can change']\n",
      "['sister you gotta have courage courage courage courage', 'so sister dont worry worry worry worry']\n",
      "['heartach Cause I dont wanna hear it', 'You gave every single heartache']\n",
      "['shame And nobody else', 'I am feeling shame']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "from string import punctuation\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf8')\n",
    "\n",
    "\n",
    "from keyword_extraction import RakeKeywordExtractor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rake = RakeKeywordExtractor()\n",
    "print(device)\n",
    "\n",
    "# Class lang: Convert each word to index and convert each index to word\n",
    "# WordEmbedding - this structure can be viewed as a corpus \n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2 #count SOS and EOS\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        if sentence:\n",
    "             for word in sentence.split(' '):\n",
    "                self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "#Prepare for training data\n",
    "#Convert all Unicode to ASCII, normalize the word: make everything lowercase, trim most punctuations\n",
    "# def unicodeToAscii(s):\n",
    "#     return ''.join(\n",
    "#         c for c in unicodedata.normalize('NFD', s)\n",
    "#         if unicodedata.category(c) != 'Mn'\n",
    "#     )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = ''.join(c for c in s if c not in punctuation)\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "#Load data, process data into pairs(pair[0] is current sentence, pair[1] is the next sentence)\n",
    "#Pair[0] is the input sentence, pair[1] is the ground truth\n",
    "def loadData():\n",
    "    pf = open('pop_eng.csv', encoding='utf-8')\n",
    "    df = pd.read_csv(pf, delimiter=',', names=['index', 'remix', 'yead', 'singer', 'type', 'lyric','keywords','scores'])\n",
    "    lyrics = df['lyric'].tolist()\n",
    "    keywords = df['keywords'].tolist()\n",
    "    scores = df['scores'].tolist()\n",
    "    print(len(keywords))\n",
    "    \n",
    "    pairs = []\n",
    "    cnt = 0\n",
    "    songs_len = len(lyrics)\n",
    "    total_line = 0\n",
    "    for i in range(1, len(lyrics)):\n",
    "        if(i%1000==0): print(i)\n",
    "#     for i in range(1, 1000):\n",
    "        lyrics_lines = lyrics[i].decode('utf-8').split('\\n')\n",
    "        keywords_lines = keywords[i].decode('utf-8').split('\\n')\n",
    "        scores_lines = scores[i].decode('utf-8').split('\\n')\n",
    "        lyric_line_num = len(lyrics_lines)\n",
    "#         keywords_line_num = len(keywords_lines)\n",
    "#         in each song\n",
    "        isFirstLine = True\n",
    "#     sometimes in the dataset there are 49 lines of lyrics, only 1 keywords\n",
    "        if(len(keywords_lines)<len(lyrics_lines)): continue\n",
    "#         print(len(lyrics_lines), len(keywords_lines))\n",
    "        for j in range(lyric_line_num):\n",
    "            total_line+=1\n",
    "            pair = []\n",
    "            current_lyric = lyrics_lines[j]\n",
    "            current_keyword = keywords_lines[j]\n",
    "            current_score = scores_lines[j]\n",
    "            if(len(current_score)>0 and round(float(current_score))>20):\n",
    "                if(isFirstLine):\n",
    "                    pair.append(normalizeString(current_keyword).encode('utf-8'))\n",
    "                    isFirstLine = False\n",
    "                else:\n",
    "                    pair.append(normalizeString(current_keyword+' '+previous_lyric).encode('utf-8'))\n",
    "                pair.append(normalizeString(current_lyric).encode('utf-8'))\n",
    "                pairs.append(pair)\n",
    "            previous_lyric = current_lyric\n",
    "    print(total_line)\n",
    "    print(len(pairs))\n",
    "    return pairs\n",
    "\n",
    "#Define a prepareData function to process all the input sentence pairs\n",
    "def prepareData(lang):\n",
    "    pairs = loadData()\n",
    "    lang = Lang(lang)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        if pair: \n",
    "            lang.addSentence(pair[0])\n",
    "            lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(lang.name, lang.n_words)\n",
    "    return lang, pairs\n",
    "\n",
    "\n",
    "lang, pairs = prepareData('lyric')\n",
    "#randomly choose a pair, print pair[0] and pair[1]\n",
    "i = 0\n",
    "while i<100:\n",
    "    print(random.choice(pairs))\n",
    "    i += 1\n",
    "\n",
    "#Define Encoder and Decoder\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "import torch.utils.data\n",
    "#Prepare for training data\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "def indexesFromWord(lang, word):\n",
    "    return [lang.word2index[word]]\n",
    "\n",
    "def tensorFromWord(lang, sentence):\n",
    "    indexes = indexesFromWord(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def evaluate_word(encoder, decoder, sentence, max_length=10):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromWord(lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "       # decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            #decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=10):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "       # decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            #decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(lang.n_words, hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, lang.n_words).to(device)\n",
    "encoder1.load_state_dict(torch.load('./encoder1_pop_e2'))\n",
    "decoder1.load_state_dict(torch.load('./decoder1_pop_e2'))\n",
    "\n",
    "def keyword_generator(sentence, num):\n",
    "    #keyword extraction\n",
    "    keywords = list(rake.sen_extract(sentence, word_num=num, incl_scores=True))\n",
    "    filter_keywords = []\n",
    "    for keyword in keywords:\n",
    "        if(keyword[1]>=10):\n",
    "            filter_keywords.append(keyword[0])\n",
    "    for keyword in filter_keywords:\n",
    "        try:\n",
    "            lang.word2index[keyword]\n",
    "        except KeyError:\n",
    "            filter_keywords.remove(keyword)\n",
    "    return filter_keywords\n",
    "\n",
    "def lyric_generator(keywords, sentence):\n",
    "    if keywords:\n",
    "        output_words = []\n",
    "        input_sen = normalizeString(sentence)\n",
    "        new_input = keywords[0] + \" \" + input_sen\n",
    "        output_word = evaluate(encoder1, decoder1, new_input)\n",
    "        output_words.append(output_word)\n",
    "        for i in range(1, len(keywords)):\n",
    "            new_sen = keywords[i] + \" \" + ' '.join(output_word)\n",
    "            sen = ' '.join(new_sen.split(' ')[:-1])\n",
    "            output_word = evaluate(encoder1, decoder1, sen)\n",
    "            output_words.append(output_word)\n",
    "\n",
    "        # encode utf-8\n",
    "        output = []\n",
    "        for sen in output_words:\n",
    "            output.append(list(map(lambda x: x, sen)))\n",
    "\n",
    "        return output\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def pop_generator(sentence):\n",
    "    keywords = keyword_generator(sentence, 4)\n",
    "    generated_lyric = lyric_generator(keywords, sentence)\n",
    "    if generated_lyric:      \n",
    "        key = []\n",
    "        for keyword in keywords:\n",
    "            key.append(keyword.encode('utf-8'))\n",
    "\n",
    "        print(\"input: \")\n",
    "        print(sentence)\n",
    "        print()\n",
    "        print(\"keywords:\")\n",
    "        print(key)\n",
    "        print()\n",
    "        print(\"output:\")\n",
    "        for out in generated_lyric:\n",
    "            print(' '.join(out))\n",
    "    else:\n",
    "        print(\"Error: We cannot extract keywords, please provide a more meaningful sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "I wanna touch ya, love ya, baby all night\n",
      "\n",
      "keywords:\n",
      "['touch', 'night', 'love', 'babi']\n",
      "\n",
      "output:\n",
      "Touch my heart touch of my mind <EOS>\n",
      "Into a day where you fall <EOS>\n",
      "but all your eyes away <EOS>\n",
      "but when i cant find <EOS>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I wanna touch ya, love ya, baby all night\"\n",
    "pop_generator(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "You can leave with me or you could have the blues\n",
      "\n",
      "keywords:\n",
      "['could', 'blue', 'leav']\n",
      "\n",
      "output:\n",
      "i love you more than you could know <EOS>\n",
      "i just cant stand at the end of your eyes\n",
      "i want a power within <EOS>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"You can leave with me or you could have the blues\"\n",
    "pop_generator(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "today is a sunny day, I am so happy\n",
      "\n",
      "keywords:\n",
      "['sunni', 'happi', 'today', 'day']\n",
      "\n",
      "output:\n",
      "Sunny each day <EOS>\n",
      "but when you want to be happy <EOS>\n",
      "but whats today today the same today <EOS>\n",
      "i used to be the same <EOS>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"today is a sunny day, I am so happy\"\n",
    "pop_generator(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: We cannot extract keywords, please provide a more meaningful sentence\n"
     ]
    }
   ],
   "source": [
    "sentence = \"ha ha ha ha ha ha\"\n",
    "pop_generator(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p27)",
   "language": "python",
   "name": "conda_pytorch_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
